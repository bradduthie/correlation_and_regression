---
title: "Regression validity"
output:
  beamer_presentation:
    theme: "default"
    colortheme: "default"
    fonttheme: "default"
  ioslides_presentation: default
  slidy_presentation: default
header-includes:
- \usepackage{hyperref}
- \usepackage{caption}
- \definecolor{links}{HTML}{2A1B81}
- \hypersetup{colorlinks,linkcolor=,urlcolor=links}
colorlinks: true
linkcolor: blue
urlcolor: blue
---

```{r, echo = FALSE}
library(knitr);
opts_chunk$set(echo = FALSE);
```


# How good is the fit of our model?

We often want to know how well a regression line fits to the data. 

- **Coefficient of determination** ($R^{2}$)
- $R^{2}$ tells us **how much of the variation in y is explained by the regression equation**
- E.g., if $R^{2} = 0.83$, then 83\% of the variation in y is accounted for by the fitted regression line
- Visually, how tightly the data points in a scatterplot fit to the regression line

See the examples below of four different $R^{2}$ values to see what this looks like.


# Scatterplots of different coefficients of determination

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Four sample regressions showing different coefficients of determination"}
par(mfrow = c(2, 2), mar = c(0, 0, 0, 0), oma = c(3, 3, 1, 1));
X1 <- rnorm(n = 24, mean = 0, sd = 1);
Y1 <- rnorm(n = 24, mean = 0, sd = 1);
while(cor(X1, Y1) < 0.199 | cor(X1, Y1) > 0.201){
  X1 <- rnorm(n = 24, mean = 0, sd = 1);
  Y1 <- rnorm(n = 24, mean = 0, sd = 1);
}
plot(x = X1, y = Y1, pch = 20, xlab = "X value", cex = 1.5, xlim = c(-3, 3),
     ylim = c(-3, 3), xaxt = "n", yaxt = "n", cex.lab = 1.25, ylab = "Y value",
     line = 1);
text(x = -1.8, y = 2.75, labels = "R^2 = 0.04", cex = 2, hjust = 0);
B0 <- as.numeric(lm(Y1~X1)$coefficients[1]);
B1 <- as.numeric(lm(Y1~X1)$coefficients[2]);
abline(a = B0, b = B1, lwd = 2);
## Row 2
X1 <- rnorm(n = 24, mean = 0, sd = 1);
Y1 <- rnorm(n = 24, mean = 0, sd = 1);
while(cor(X1, Y1) > -0.399 | cor(X1, Y1) < -0.401){
  X1 <- rnorm(n = 24, mean = 0, sd = 1);
  Y1 <- rnorm(n = 24, mean = 0, sd = 1);
}
plot(x = X1, y = Y1, pch = 20, xlab = "X value", cex = 1.5, xlim = c(-3, 3),
     ylim = c(-3, 3), xaxt = "n", yaxt = "n", cex.lab = 1.25, ylab = "Y value",
     line = 1);
text(x = -1.7, y = 2.75, labels = "R^2 = 0.16", cex = 2, hjust = 0);
B0 <- as.numeric(lm(Y1~X1)$coefficients[1]);
B1 <- as.numeric(lm(Y1~X1)$coefficients[2]);
abline(a = B0, b = B1, lwd = 2);
## Row 3
X1 <- rnorm(n = 24, mean = 0, sd = 1);
Y1 <- rnorm(n = 24, mean = 0, sd = 1);
while(cor(X1, Y1) < 0.699 | cor(X1, Y1) > 0.701){
  X1 <- rnorm(n = 24, mean = 0, sd = 1);
  Y1 <- rnorm(n = 24, mean = 0, sd = 1);
}
plot(x = X1, y = Y1, pch = 20, xlab = "X value", cex = 1.5, xlim = c(-3, 3),
     ylim = c(-3, 3), xaxt = "n", yaxt = "n", cex.lab = 1.25, ylab = "Y value",
     line = 1);
text(x = -1.8, y = 2.75, labels = "R^2 = 0.49", cex = 2, hjust = 0);
B0 <- as.numeric(lm(Y1~X1)$coefficients[1]);
B1 <- as.numeric(lm(Y1~X1)$coefficients[2]);
abline(a = B0, b = B1, lwd = 2);
## Row 2
X1 <- rnorm(n = 24, mean = 0, sd = 1);
Y1 <- X1;
plot(x = X1, y = Y1, pch = 20, xlab = "X value", cex = 1.5, xlim = c(-3, 3),
     ylim = c(-3, 3), xaxt = "n", yaxt = "n", cex.lab = 1.25, ylab = "Y value",
     line = 1);
text(x = -1.9, y = 2.75, labels = "R^2 = 1", cex = 2, hjust = 0);
mtext(text="X value",side=1,line=1,outer=TRUE, cex = 1.5);
mtext(text="Y value",side=2,line=1,outer=TRUE, cex = 1.5);
B0 <- as.numeric(lm(Y1~X1)$coefficients[1]);
B1 <- as.numeric(lm(Y1~X1)$coefficients[2]);
abline(a = B0, b = B1, lwd = 2);
```



# More understand of the coefficient of determination

Understanding  that **the coefficient of determination tells us how much variation in y is explained by the regression equation** is the important point. 

$$R^{2} = 1- \frac{SS_{res}}{SS_{tot}}.$$


- Coefficient of determination compares the sum of squared residuals from the linear model ($SS_{res}$) to what the sum of squared residuals would be if we had just use the mean value of y ($SS_{tot}$).

- Conveniently, $R^{2}$ is also just the Pearson product moment correlation (*r*) squared.


# Visualising the coefficient of determination

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "A regression of one dependent variable y against the independent variable x. Blue vertical lines show residuals of the linear model, while red dotted vertical lines show residual deviations of from the mean of y."}
X1 <- c(2.02, 8.6, 4.9, 1.5, 2.8, 4.6, 7.5, 6.2, 3.3, 5.6);
Y1 <- c(5, 3, 6, 1.3, 5.5, 3.1, 5.2, 6.8, 3.6, 6.6);
B0 <- as.numeric(lm(Y1~X1)$coefficients[1]);
B1 <- as.numeric(lm(Y1~X1)$coefficients[2]);
resi <- lm(Y1~X1)$residuals;
mxrs <- which(X1 == max(X1));
plot(x = X1, y = Y1, pch = 20, xlab = "Independent Variable X", cex = 1.5, 
     xlim = c(0, 10), ylim = c(0, 10), cex.lab = 1.25, cex.axis = 1.25,
     ylab = "Dependent Variable Y", yaxs = "i", xaxs = "i");
abline(a = B0, b = B1, lwd = 2, col = "blue");
abline(h = mean(Y1), col = "red", lty = "dashed");
for(i in 1:length(X1)){
  points(x = c(X1[i] - 0.03, X1[i] - 0.03), y = c(Y1[i], B0 + X1[i]*B1), 
         col = "blue", lwd = 1, type = "l");
  points(x = c(X1[i] + 0.03, X1[i] + 0.03), y = c(Y1[i], mean(Y1)), col = "red",
         lwd = 1, lty = "dotted", type = "l");
}
points(x = X1, y = Y1, pch = 20, xlab = "Independent Variable X", cex = 1.5, 
     xlim = c(0, 10), ylim = c(0, 10), cex.lab = 1.25, cex.axis = 1.25,
     ylab = "Dependent Variable Y", yaxs = "i", xaxs = "i");
text(x = 2, hjust = 0, y = 9, labels = "SS_res = 26.73", cex = 1.5, 
     col = "blue");
text(x = 2, hjust = 0, y = 8, labels = "SS_tot = 28.82", cex = 1.5, 
     col = "red");
```


# The F-test of overall significance


- Test the significance of our overall regression model using an F-test of overall significance
- Determines whether or not our linear regression provides a better fit to the data than a model that does not contain any independent variables 

**Hypothesis for F-test of overall significance of a linear model**

- **Null:** The model with no independent variables fits the data as well as the linear model
- **Alternative:** The linear model fits the data better than the model with no independent variables

Understand is how to interpret the F-test of overall significance (see below for doing this in practice).


# Significance of equation parameters


- Test the significance of individual parameters in the linear model (a and b; recall that y = a + bx).
- For both coefficients a and b, we can state the null and alternative hypotheses

**Hypothesis for coefficient of a linear model**

- **Null:** The value of the coefficient equals 0.
- **Alternative:** The value of the coefficient does not equal 0.

Statistical software such as SPSS will calculate p-values to test our null hypothesis for both a and b coefficients. Lecture notes include the details of how this is done (you do not need to know these details).


# Assessing the practical validity of regression

The practical validity of the regression model is assessed by comparing the predicted values with the observed data. We can do this in several ways:

1. Plotting the fitted regression line and checking that the observed data lie close to the line (i.e., high coefficient of determination).
2. Plotting observed versus predicted values and observing a linear relationship between the independent and dependent variable.
3. Examining the data for large residuals (i.e., outliers), which might be distorting the regression line.
4. Ideally, test the regression model on new observational data to examine how close the predicted values are to the observations

# Prediction with linear regression

Regression equations can be used to calculate additional y values when values of x are substituted in a regression equation.

- **Interpolation**: Predictions made within the measurement range of the data
- **Extrapolation**: Predictions made outside the measurement range of the data

**Care should be taken when extrapolating beyond the measured data because the relationship between the two variables might change.**


# Predictions with linear regression

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "A regression of one dependent variable y against the independent variable x."}
X1 <- c(2.02, 8.6, 4.9, 1.5, 2.8, 4.6, 7.5, 6.2, 3.3, 5.6);
Y1 <- c(5, 3, 6, 1.3, 5.5, 3.1, 5.2, 6.8, 3.6, 6.6);
B0 <- as.numeric(lm(Y1~X1)$coefficients[1]);
B1 <- as.numeric(lm(Y1~X1)$coefficients[2]);
resi <- lm(Y1~X1)$residuals;
mxrs <- which(X1 == max(X1));
plot(x = X1, y = Y1, pch = 20, xlab = "Independent Variable X", cex = 1.5, 
     xlim = c(0, 10), ylim = c(0, 10), cex.lab = 1.25, cex.axis = 1.25,
     ylab = "Dependent Variable Y", yaxs = "i", xaxs = "i");
abline(a = B0, b = B1, lwd = 2);
```








